{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1-) outlier lar nasıl uğraşılır ?  Z score, quartile lof (mert) \n",
        "\n",
        "Xgboost outlier’a karşı duyarlı outlier handle edilmesi gerekiyor \n",
        "\n",
        "2-) Prophet kütüphanesi çalışma mantığı ? (selin) \n",
        "\n",
        "3-)  Mean Absolute Scaled Error (MASE) regression score olarak diğer score ların nasıl hesaplandığı ? 2-3 tane yükseklik düşüklük değerleri nedir? (nermin) \n",
        "\n",
        "4-) time series da kullanılan featurelar neler olabilir? Feature engineering for time series  \n",
        "\n",
        "Several deliberate features are calculated includes: rolling feature, lag feature, selling price feature as well as min, max, median features. Then we combine these features together. (onur ) \n",
        "\n",
        "5-) feature sayısının çok artması overfittinge sebep olur mu ? Optimal feature sayısı ne olmalı(mikail) \n",
        "\n",
        "6-) xgboost penalize nasıl yapıyor ? Overfitting i engellemek için mi ? (onur) \n",
        "\n",
        "7-) xgboost shrinkage, column sub sampling bu özellikleri açıklayalım?     \n",
        "\n",
        "XGBoost applies two additional techniques to further reduce over­ fitting. The first is shrinkage [14], which reduces the influence of each individual tree and leaves space for future trees to improve the model by scaling newly added weights by a factor coefficient after each step of tree boosting. The second technique is column sub-sampling, which prevents over-fitting more than the traditional row sub-sampling. (onur) \n",
        "\n",
        "8-) xgboost özellikleri neler ?   \n",
        "\n",
        "an approximate algorithm for exact greedy algorithm; \n",
        "\n",
        "storing the data in in-memory units for parallel learning; \n",
        "\n",
        "leveraging a cache-aware prefetching algorithm;  \n",
        "\n",
        "enabling out-of-core computation. (mikail) \n",
        "\n",
        "9- RMSSE (scaling) nedir rmse farkı ne ? (mustafa) "
      ],
      "metadata": {
        "id": "JZMCcgS2uUZL"
      }
    }
  ]
}
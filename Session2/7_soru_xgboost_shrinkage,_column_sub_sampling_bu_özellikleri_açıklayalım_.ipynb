{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "7 - ) xgboost shrinkage, column sub sampling bu özellikleri açıklayalım?\n",
        "\n",
        "A problem with gradient boosted decision trees is that they are quick to learn and overfit training data.\n",
        "\n",
        "One effective way to slow down learning in the gradient boosting model is to use a learning rate, also called shrinkage (or eta in XGBoost documentation).\n",
        "\n",
        "Smaller learning rates generally require more trees to be added to the model.(increase n_estimators)\n",
        "\n",
        "https://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/\n",
        "\n",
        "https://xgboost.readthedocs.io/en/stable/parameter.html\n",
        "\n",
        "the learning rate (eta) must be set as low as possible. However, as the learning rate (eta) gets lower, you need many more steps (rounds) to get to the optimum:\n",
        "\n",
        "Increasing eta makes computation faster (because you need to input less rounds) but does not make reaching the best optimum. Decreasing eta makes computation slower (because you need to input more rounds) but makes easier reaching the best optimum.\n",
        "\n",
        "https://medium.com/data-design/let-me-learn-the-learning-rate-eta-in-xgboost-d9ad6ec78363\n",
        "\n",
        "learning rate: It affects how quickly the model fits the residual error using additional base learners. A low learning rate will require more boosting rounds to achieve the same reduction in residual error as an XGBoost model with high learning rate. It is denoted by eta.\n",
        "\n",
        "gamma: It is the minimum loss reduction to create new tree-split. To make the algorithm more conservative, the high value of gamma is preferred. lambda: This is responsible for L2 regularization on leaf weights.\n",
        "\n",
        "alpha: This is responsible for L1 regularization on leaf weights.\n",
        "\n",
        "max_depth: It is a positive integer value, and is responsible for how deep each tree will grow during any boosting round.\n",
        "\n",
        "sub_sample: It ranges from 0 to 1 and is the fraction of total training set that can be used for any given boosting round. The low value of this parameter may lead to unfitting problems and high values may lead to overfitting.\n",
        "\n",
        "colsample_bytree: This parameter also ranges from 0 to 1. It is the fraction of features that can be selected during any given boosting rounds.\n",
        "\n",
        "https://towardsdatascience.com/fine-tuning-xgboost-model-257868cf4187\n",
        "\n",
        "Sub sample. Sub sample is the ratio of the training instance. For example, if you set this to 0.5, XGBoost will randomly collect half the data instances to grow trees and this will prevent overfitting.\n",
        "\n",
        "Eta. The step size shrinkage used during the update step to prevent overfitting. After each boosting step, the weights of new features can be obtained directly. Eta also shrinks the feature weights to make the boosting process more conservative.\n",
        "\n",
        "Gamma. The minimum loss reduction required to make a further partition on a leaf node of the tree. The larger the gamma setting, the more conservative the algorithm will be.\n",
        "\n",
        "Colsample by tree. Sub sample ratio of columns when constructing each tree.\n",
        "\n",
        "Colsample by level. Sub sample ratio of columns for each split, in each level.\n",
        "\n",
        "Lambda. L2 regularization term on weights. Increasing this value will make the model more conservative.\n",
        "\n",
        "Alpha. L1 regularization term on weights Increasing this value will make model more conservative.\n",
        "\n",
        "https://www.ibm.com/docs/pl/spss-modeler/18.1.0?topic=node-xgboost-tree-build-options\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
      ],
      "metadata": {
        "id": "sDjGOHOnyoqR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLR7Pfw2yoMr"
      },
      "outputs": [],
      "source": []
    }
  ]
}
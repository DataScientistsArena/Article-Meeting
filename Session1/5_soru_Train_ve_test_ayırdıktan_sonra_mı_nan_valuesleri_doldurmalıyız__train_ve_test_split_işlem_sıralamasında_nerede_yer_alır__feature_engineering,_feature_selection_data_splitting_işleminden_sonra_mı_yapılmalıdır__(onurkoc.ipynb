{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Train ve test ayırdıktan sonra mı nan valuesleri doldurmalıyız? train ve test split işlem sıralamasında nerede yer alır? feature engineering, feature selection data splitting işleminden sonra mı yapılmalıdır? (onurkoc83)  "
      ],
      "metadata": {
        "id": "SqZlf7u8sDn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should do the same preprocessing on all your data however if that preprocessing depends on the data (e.g. standardization, pca) then you should calculate it on your training data and then use the parameters from that calculation to apply it to your validation and test data.\n",
        "\n",
        "For example if you are centering your data (subtracting the mean) then you should calculate the mean on your training data ONLY and then subtract that same mean from all your data (i.e. subtract the mean of the training data from the validation and test data, DO NOT calculate 3 separate means).\n",
        "\n",
        "For cross-validation, you'll have to calculate it for each iteration on the folds in the training set and then apply that calculation to the validation fold. If you then train a model using all your data after that, then you need to find the parameters for the preprocessing step using all the CV data.\n",
        "\n",
        "https://stats.stackexchange.com/questions/321559/pre-processing-applied-on-all-three-training-validation-test-sets"
      ],
      "metadata": {
        "id": "Onfk6AeNsDmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To add to phanny's answer - you should do the preprocessing on your training set separately, otherwise information from the test set will \"leak\" into your training data.\n",
        "\n",
        "The test set should ideally not be preprocessed with the training data. This will ensure no 'peeking ahead'. Train data should be preprocessed separately and once the model is created we can apply the same preprocessing parameters used for the train set, onto the test set as though the test set didn't exist before.\n",
        "\n",
        "https://stats.stackexchange.com/questions/267012/difference-between-preprocessing-train-and-test-set-before-and-after-splitting\n",
        "\n"
      ],
      "metadata": {
        "id": "cgvkrpRcsDjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should not use a preprocessing method that is fitted on the whole dataset, to transform the test or train data.\n",
        "\n",
        "If you do so, you are inadvertently carrying information from the train set over to the test set.\n",
        "\n",
        "we scale our features, only after the train test split\n",
        "\n",
        "https://towardsdatascience.com/3-things-you-need-to-know-before-you-train-test-split-869dfabb7e50"
      ],
      "metadata": {
        "id": "arugBbIWsDhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should split before pre-processing or imputing.\n",
        "\n",
        "The division between training and test set is an attempt to replicate the situation where you have past information and are building a model which you will test on future as-yet unknown information: the training set takes the place of the past and the test set takes the place of the future, so you only get to test your trained model once.\n",
        "\n",
        "Keeping the past/future analogy in mind, this means anything you do to pre-process or process your data, such as imputing missing values, you should do on the training set alone. You can then remember what you did to your training set if your test set also needs pre-processing or imputing, so that you do it the same way on both sets.\n",
        "\n",
        "Added from comments: if you use the test data to affect the training data, then the test data is being used to build your model, so it ceases to be test data and will not provide a fair test of your model. You risk overfitting, and it was to discourage this that you separated out the test data in the first place\n",
        "\n",
        "**\n",
        "\n",
        "Just to add on the above I would also favour spliting before imputing or any type of pre-processing. Nothing you do with the training data should be informed by the test data (the analogy is that the future should not affect the past). You can then remember what you did to your training set if your test set also needs pre-processing or imputing, so that you do it the same way on both sets (the analogy is that you can use the past to help predict the future).\n",
        "\n",
        "If you use the test data to affect the training data in any way, then the test data is being used to build your model, so it ceases to be test data and will not provide a fair test of your model. You risk over fitting, and it was to discourage this that you separated out the test data in the first place!\n",
        "\n",
        "https://stats.stackexchange.com/questions/95083/imputation-before-or-after-splitting-into-train-and-test"
      ],
      "metadata": {
        "id": "uySwHvdbsDfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection\n",
        "split first, and select the features based on the training set only\n",
        "\n",
        "https://stackoverflow.com/questions/56308116/should-feature-selection-be-done-before-train-test-split-or-after"
      ],
      "metadata": {
        "id": "atOf8z8qsSel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You first need to split the data into training and test set (validation set could be useful too).\n",
        "\n",
        "Don't forget that testing data points represent real-world data. Feature normalization (or data standardization) of the explanatory (or predictor) variables is a technique used to center and normalise the data by subtracting the mean and dividing by the variance. If you take the mean and variance of the whole dataset you'll be introducing future information into the training explanatory variables (i.e. the mean and variance).\n",
        "\n",
        "Therefore, you should perform feature normalisation over the training data. Then perform normalisation on testing instances as well, but this time using the mean and variance of training explanatory variables. In this way, we can test and evaluate whether our model can generalize well to new, unseen data points.\n",
        "\n",
        "https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data"
      ],
      "metadata": {
        "id": "32S2P--UsScQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoYcCyGZr5dO"
      },
      "outputs": [],
      "source": []
    }
  ]
}